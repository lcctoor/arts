# 问题起源

今天在写一个自用的同步文件工具，采用哈希值作为文件指纹。发现当文件容量较大时，“读取文件-计算哈希”这一过程较久，故对大文件使用“文件容量+修改时间+抽样哈希”综合作为指纹。所用抽样哈希算法为：从文件中均匀取 n 个点，再以取出的 n 个点计算哈希值。

这引发我产生了 2 个问题：

1、当抽样点数和哈希值长度一样时，直接使用样点作为指纹和使用哈希值作为指纹哪个更准确？

2、设哈希值长度为 16 字节，计算完整文件的哈希值作为指纹和从文件均匀取 16 个点作为指纹哪个更准确？

# 解答问题1

哈希值是原数据（样点）的有损映射，故直接使用样点作为指纹比使用哈希值作为指纹更准确。

# 解答问题2

哈希值是原文件的有损映射，均匀抽样也是原文件的有损映射，需要进一步分析。

实际上哈希值是一个统称，不同的哈希算法会产生不同的哈希值，这个问题需要看具体的哈希算法。实际上，均匀抽样也是一种哈希算法，因此这个问题的本质是：两个不同的哈希算法哪个准确性更高。

对于哈希值长度固定的哈希算法，由于哈希值长度有限以及 每个比特位的可能值（0 和 1）有限 ，因此原数据与哈希值是多对一关系，当文件数量足够多时，每个哈希值将对应一组（若干个）文件。

在现实中，我们电脑中可能有大量的相似文件：好听的音乐大概率集中在某些狭窄的音域上、一张图片可能存在若干 滤镜\|尺寸\|格式 不同的版本、一个文本被修改一两个字符就是一个新文件。

对于同步文件这个目的，一个好的哈希算法应该使原数据的相关性（相关但不相等）不能导致哈希值的相关性，使哈希碰撞概率均匀分散在值域中的每个值上，这样才能在大量相似但不相同的文件中降低哈希碰撞的总概率。

对于同步文件这个目的，一个好的哈希算法（H）应该具备这个特点（J）：对于任意正整数（L），随机生成一条L比特的数据（N），为 N 创建 L 个副本，分别对每个副本上的 1 个不同位置做取反，以得到 L 条新数据，这 L 条数据两两之间都有且只有 2 个位置上的值不相等，用 H 分别对这 L 条数据计算哈希值以得到 L 个哈希值，则这 L 个哈希值在 H 的值域中接近均匀分布。

均匀抽样由于存在 抽样间隔 ，因此不符合上述特点 J ，故均匀抽样对于同步文件是一个不好的哈希算法。考虑到市面上时下流行的哈希算法都是严谨设计的，故直接估计比均匀抽样更符合特点 J 。

## 结论

设哈希值长度为 16 字节，计算完整文件的哈希值作为指纹比从文件均匀取 16 个点作为指纹更准确。
